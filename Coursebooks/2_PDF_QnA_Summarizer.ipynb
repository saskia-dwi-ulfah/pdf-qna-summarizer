{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6093b47",
   "metadata": {},
   "source": [
    "**Coursebook: Developing a PDF Summarizer and Q&A System**\n",
    "\n",
    "- Course Length: 6 hours\n",
    "- Last Updated: May 2024\n",
    "\n",
    "___\n",
    "\n",
    "Developed by Algoritma's Product Team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089054",
   "metadata": {},
   "source": [
    "# Developing a PDF Summarizer and Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cafca3",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In today's digital age, the volume of information stored in PDF documents has exponentially grown across various industries, including education, research, legal, and corporate sectors. PDF files serve as an essential medium for sharing, archiving, and disseminating knowledge. However, the sheer volume and complexity of these documents often make it challenging for users to extract relevant information efficiently.\n",
    "\n",
    "**Summarizer Challenge:** \n",
    "Users often face long PDFs with both important and irrelevant details. Reading and summarizing these manually takes time and can lead to mistakes. PDFs can also have various content types like text, tables, images, and graphs, making it tricky to summarize them neatly. Plus, everyone's needs for summaries are different, so a one-size-fits-all approach might not work well.\n",
    "\n",
    "**Q&A System Challenge:** \n",
    "Understanding what users are asking about in relation to PDF content can be tough due to language differences and context. To give users the right answers, the system needs to pull the relevant info from the PDFs quickly and accurately. The answers also need to be spot-on to build trust and satisfaction with the system.\n",
    "\n",
    "**LLM Integration:** \n",
    "Using a large language model (LLM) like GPT-3 can help tackle these challenges. LLMs are great at understanding language, summarizing text, and answering questions. But to make this work with PDFs, we need to:\n",
    "\n",
    "1. **Document Pre-processing:** Prepare the PDF content for LLMs by converting it into a usable format, handling text from images or tables, and keeping the document's structure intact.\n",
    "  \n",
    "2. **Fine-tuning and Customization:** Adjust the LLM to better suit specific topics or user needs. This makes the summarization and Q&A features more accurate.\n",
    "  \n",
    "3. **Scalability and Efficiency:** Make sure the system can manage lots of PDFs smoothly and respond to user questions quickly.\n",
    "\n",
    "To solve these issues, we'll need expertise in natural language processing, machine learning, document handling, and user interface design. Creating a PDF summarizer and Q&A system using an LLM could greatly improve how we access and manage information across many fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0ae01",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb23662",
   "metadata": {},
   "source": [
    "The objective of this coursebook is to provide learners with a comprehensive understanding and practical skills in working with PDF files and Large Language Models (LLMs) for developing a Q&A system and summarizer.\n",
    "\n",
    "**Coursebook Outline:**\n",
    "\n",
    "1. **Introduction to PDF File:**\n",
    "   - Learn what PDF files are and why they're important.\n",
    "   - Find out how to open and use PDF files in code.\n",
    "\n",
    "2. **Introduction to LLM:**\n",
    "   - Get to know Large Language Models like GPT-3, GPT-2, and BERT.\n",
    "   - Understand what LLMs can and can't do in language tasks.\n",
    "   - Learn about LangChain for using LLMs effectively.\n",
    "\n",
    "3. **Text Preprocessing:**\n",
    "   - Learn basic steps to get text ready for analysis.\n",
    "   - Understand how to clean and organize text.\n",
    "\n",
    "4. **Extracting Text Using Vector Database:**\n",
    "   - Use Chroma to pull text from different sources, including PDFs.\n",
    "   - Set up API keys and handle environment settings with .env files.\n",
    "   - Practice extracting text with Chroma.\n",
    "\n",
    "5. **Q&A System and Summarizer Development:**\n",
    "   - Build a Q&A system that uses LLMs to answer questions from PDFs.\n",
    "   - Create a summarization tool to make short summaries of PDFs with LLMs.\n",
    "\n",
    "By the end of this coursebook, you'll know how to use these tools to get information from PDF files and make Q&A and summarization systems using Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3ecd1",
   "metadata": {},
   "source": [
    "# 1. Introduction to PDF File\n",
    "\n",
    "PDF stands for Portable Document Format. It's a file format developed by Adobe that captures all the elements of a printed document as an electronic image that can be viewed, printed, or transmitted easily. PDF files are widely used because they preserve the formatting, fonts, and layout of the original document, making them ideal for sharing documents across different platforms and devices without losing their appearance.\n",
    "\n",
    "PDF files have become essential in today's digital world for several reasons:\n",
    "\n",
    "- **Universal Compatibility:** PDFs can be opened and viewed on virtually any device and operating system using free software like Adobe Acrobat Reader, making them universally accessible.\n",
    "  \n",
    "- **Document Preservation:** Unlike other file formats, PDFs preserve the original layout, fonts, and graphics of a document, ensuring that it looks the same regardless of where or how it's viewed.\n",
    "  \n",
    "- **Security Features:** PDFs can be encrypted and password-protected, allowing users to control who can access, edit, or print the document.\n",
    "  \n",
    "- **Multi-page Support:** PDFs can contain multiple pages, making them suitable for creating reports, presentations, and ebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a17c1d",
   "metadata": {},
   "source": [
    "**Opening and Using PDF Files in Python:**\n",
    "\n",
    "To work with PDF files in Python, we can use libraries that provide functionalities to manipulate PDF documents. One popular library for this purpose is `PyPDF2`. Here's a guide on how to open and use PDF files in Python using `PyPDF2`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1805420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdf_file_path = \"data_input/bei_annual_report_2022.pdf\"\n",
    "loader = PdfReader(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adc66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\n",
    "\n",
    "for page in loader.pages:\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8820d9",
   "metadata": {},
   "source": [
    "When running the above code, we instantiate the object to read the PDF document. This object will extract every page of the PDF document. We iterate through each page and extract the text contained within."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc58b4",
   "metadata": {},
   "source": [
    "# 2. Introduction to Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563a39c",
   "metadata": {},
   "source": [
    "**Large Language Models (LLMs)** like GPT-3 offer powerful capabilities in natural language processing, making them well-suited for tasks involving **text analysis, summarization, and question answering**. Their ability to understand and generate human-like text can greatly enhance the efficiency and accuracy of systems that work with textual data. By integrating LLMs into our task of **PDF summarization and Q&A system development**, we can leverage their advanced language understanding capabilities to create more intelligent and effective solutions.\n",
    "\n",
    "\n",
    "**What is LLM?**\n",
    "\n",
    "A Large Language Model (LLM) is a type of artificial intelligence model trained on vast amounts of text data to understand and generate human-like text. LLMs, such as GPT-3 (Generative Pre-trained Transformer 3), are designed to perform various natural language processing tasks, including text generation, translation, summarization, and question answering, among others. These models learn from the patterns and structures in the data they are trained on, allowing them to generate coherent and contextually relevant text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6517604",
   "metadata": {},
   "source": [
    "**History of LLM**\n",
    "\n",
    "The development of Large Language Models has been a significant milestone in the field of artificial intelligence and natural language processing. The history of LLMs can be traced back to earlier language models like recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). However, the breakthroughs in transformer architectures, particularly with models like GPT (Generative Pre-trained Transformer), have led to the development of more powerful and scalable LLMs.\n",
    "\n",
    "The evolution of LLMs has been marked by advancements in training techniques, model architectures, and data sources. With each iteration, these models have become larger, more capable, and better at understanding and generating human-like text, driving innovations across various applications and industries.\n",
    "\n",
    "**Understand What LLMs Can and Can't Do in Language Tasks**\n",
    "\n",
    "LLMs excel at many natural language processing tasks, thanks to their ability to understand context, generate coherent text, and perform complex language tasks. Here are some tasks LLMs are good at:\n",
    "\n",
    "- **Text Generation:** Generating human-like text based on the input and context.\n",
    "  \n",
    "- **Translation:** Translating text between different languages with reasonable accuracy.\n",
    "  \n",
    "- **Summarization:** Creating concise summaries of longer texts.\n",
    "  \n",
    "- **Question Answering:** Providing relevant answers to questions based on the input text.\n",
    "\n",
    "However, LLMs also have limitations:\n",
    "\n",
    "- **Context Understanding:** While they are good at understanding context within a single passage, they may struggle with broader or multi-document contexts.\n",
    "  \n",
    "- **Fact-checking:** They may generate plausible but incorrect information if not guided by accurate data.\n",
    "  \n",
    "- **Ethical Considerations:** LLMs can sometimes generate biased or inappropriate content if not carefully controlled and monitored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6ff76",
   "metadata": {},
   "source": [
    "## LangChain ü¶úüîó\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing applications powered by language models that refers to the integration of multiple language models and APIs to create a powerful and flexible language processing pipeline. It involves connecting different language models, such as OpenAI's GPT-3 or GPT-2, with other tools and APIs to enhance their functionality and address specific business needs. \n",
    "\n",
    "The LangChain concept aims to leverage the strengths of each language model and API to create a comprehensive language processing system. It allows developers to combine different models for tasks like question answering, text generation, translation, summarization, sentiment analysis, and more.\n",
    "\n",
    "In the context of our task, LangChain could involve using a combination of LLMs for different stages of PDF summarization and Q&A system development. For example, one LLM could be used for text extraction and preprocessing, while another could handle question answering and summarization. By chaining these models together effectively, we can leverage their complementary strengths to create a more powerful and efficient system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d42f0f",
   "metadata": {},
   "source": [
    "# 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45e782",
   "metadata": {},
   "source": [
    "Previously, we have read and assigned the content of the PDF file to the `raw_text` variable. Let's inspect the length of characters within this variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1ceab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440092"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022563b",
   "metadata": {},
   "source": [
    "Our loaded PDF content contains more than 1 million characters. This huge amount of data requires us to conduct additional processing to make it easier to provide the information within the PDF document to the LLM. We will preserve this raw text into a more manageable form by chunking it into smaller units. To achieve this, we will use the `CharacterTextSplitter()` object from the `langchain.text_splitter` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6239c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator = \"\\n\", \n",
    "                                      chunk_size = 1000, \n",
    "                                      chunk_overlap = 10, \n",
    "                                      length_function = len)\n",
    "text = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5ef444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb272ec",
   "metadata": {},
   "source": [
    "> Visit [this documentation](https://python.langchain.com/docs/modules/data_connection/document_transformers/character_text_splitter/) for more information about `CharacterTextSplitter()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b0e4b",
   "metadata": {},
   "source": [
    "`CharacterTextSplitter()` is the most straightforward method Langchain provides for processing text data. The splitting process of the raw text into chunks is based on single characters. In our case, it is based on the newline character (`\\n`). Furthermore, the size of the chunk is measured by the number of characters in each chunk. In our code, we set the chunk size to be 1000 characters.\n",
    "\n",
    "By the end of this process, we will have 1500 chunks, each with around 1000 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0c474",
   "metadata": {},
   "source": [
    "# 4. Extracting Text Using Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524c32f",
   "metadata": {},
   "source": [
    "When working with PDF files, we often deal with a large amount of unstructured data. To efficiently handle and retrieve information from such data, we need a structured and optimized approach. This is where **vector databases** come into play.\n",
    "\n",
    "**Vector databases** are specialized databases designed to store and manipulate vector data efficiently. In the context of text extraction from PDF files, vector databases provide a structured storage mechanism that allows us to store text data in a way that facilitates quick and accurate retrieval.\n",
    "\n",
    "**Why Use Chroma for Text Extraction?**\n",
    "\n",
    "Chroma is a powerful tool designed to extract text from various sources, including PDF files. Its advanced algorithms and features make it particularly well-suited for dealing with unstructured data like the content found in PDF documents. Here's why we use Chroma:\n",
    "\n",
    "1. **Efficiency:** Chroma is highly efficient at extracting text from PDF files, even when dealing with large volumes of data. Its optimized algorithms ensure fast processing times, allowing us to extract text quickly and effectively.\n",
    "\n",
    "2. **Accuracy:** Chroma provides accurate text extraction results, minimizing errors and ensuring the reliability of the extracted information. This is crucial, especially when dealing with important or sensitive data contained within PDF documents.\n",
    "\n",
    "3. **Versatility:** Chroma is capable of extracting text from various sources, including PDFs, images, scanned documents, and more. Its versatility makes it a valuable tool for handling different types of unstructured data and extracting valuable insights from them.\n",
    "\n",
    "\n",
    "The specific vector database that we will use is the **ChromaDB** vector database.\n",
    "\n",
    "> Visit [this website](https://docs.trychroma.com/getting-started#:~:text=Chroma%20is%20a%20database%20for,hosted%20version%20is%20coming%20soon!) for more information about Chroma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef13356",
   "metadata": {},
   "source": [
    "As a vector database, Chroma stores embeddings that represent various types of data, including text and images. In simple terms, embeddings convert our data into a format that is processable by computers (numbers). In the following code, we will perform actions to convert the previous chunks into numerical representations (embeddings) and store them in Chroma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef3bcf",
   "metadata": {},
   "source": [
    "In preparation for interfacing Chroma functionalities using LangChain, we will undertake the following prerequisite step: environment setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7b5cb",
   "metadata": {},
   "source": [
    "## Environment Set-up\n",
    "\n",
    "Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n",
    "\n",
    "### Setting API key and `.env`\n",
    "\n",
    "Accessing the API requires an API key, which you can get by creating an account and heading here. When setting up an API key and using a .env file in your Python project, you follow these general steps:\n",
    "\n",
    "1. **Obtain an API key**: If you're working with an external API or service that requires an API key, you need to obtain one from the provider. This usually involves signing up for an account and generating an API key specific to your project.\n",
    "\n",
    "2. **Create a .env file**: In your project directory, create a new file and name it \".env\". This file will store your API key and other sensitive information securely.\n",
    "\n",
    "3. **Store API key in .env**: Open the .env file in a text editor and add a line to store your API key. The format should be `API_KEY=your_api_key`, where \"API_KEY\" is the name of the variable and \"your_api_key\" is the actual value of your API key. Make sure not to include any quotes or spaces around the value.\n",
    "\n",
    "4. **Load environment variables**: In your Python code, you need to load the environment variables from the .env file before accessing them. Import the dotenv module and add the following code at the beginning of your script:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "> `dotenv` library is a popular Python library that simplifies the process of loading environment variables from a .env file into your Python application. It allows you to store configuration variables separately from your code, making it easier to manage sensitive information such as API keys, database credentials, or other environment-specific settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b93d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15088236",
   "metadata": {},
   "source": [
    "This output verifies that our OpenAI API key is successfully loaded. We don't need to pass any arguments regarding the API key when calling certain functionalities related to OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a201336",
   "metadata": {},
   "source": [
    "## Embedding and Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354884ab",
   "metadata": {},
   "source": [
    "Before storing the chunks of information in Chroma, we need to define which embedding model we will utilize to convert the text to the vector or its numerical representation. Since we will use the chat model developed by OpenAI, we will use the embedding model from the same source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b759eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model = \"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b4b42",
   "metadata": {},
   "source": [
    "Chroma, via LangChain, provides a convenient way to store text embeddings. We simply pass the texts that we intend to convert to vectors and the embedding function. This process may take some time, depending on the thickness of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a457d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_texts(text, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c4f68",
   "metadata": {},
   "source": [
    "Our PDF document has been converted into embeddings in `vectordb`. Now, we will provide this `vectordb` as additional knowledge for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed0bef",
   "metadata": {},
   "source": [
    "# 5. Q&A System and Summarizer Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704bfa2",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00ec15",
   "metadata": {},
   "source": [
    "When developing a Q&A system with LangChain, we require several basic building blocks to construct a unified chain. This chain will receive an input and pass it through every component of the chain to ultimately achieve the desired outcome.\n",
    "\n",
    "To further understand each of these components, let's import the necessary tools to build a Q&A chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9671c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "# passing the query\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# parsing the output\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "220a2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences minimum and give the answer in the complete way.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70269a1a",
   "metadata": {},
   "source": [
    "The first component of the chain is the prompt. We can think of the prompt as an instruction on what output should be generated by the LLM. In the case of creating a Q&A system, we only want the LLM to answer based on the provided knowledge context. In this case, the LLM does not use general knowledge that it already possesses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f3f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo-0125', temperature = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806aa929",
   "metadata": {},
   "source": [
    "The second component of the chain is the LLM. The LLM will perform the information retrieval task based on the question it receives and generate an appropriate response based on the defined prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bdb254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c13ee",
   "metadata": {},
   "source": [
    "The third component of our Q&A chain is the retriever. A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents; it only needs to return (or retrieve) them. This component plays an important role as it provides the base knowledge for the LLM. The query passed to our chain will be computed in similarity with the additional context in the database to enable the return of appropriate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de70ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fc2e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2217378",
   "metadata": {},
   "source": [
    "Finally, the components of the Q&A chain will be declared using LangChain Expression Language, or LCEL. The `|` symbol is similar to a Unix pipe operator, which chains together the different components, feeding the output from one component as input into the next component.\n",
    "\n",
    "* We pass in a query (question) we would like to ask about the PDF document.\n",
    "* The prompt component takes the user input, which is then used to construct a `PromptValue` after using the query to construct the prompt.\n",
    "* The model component takes the generated prompt and passes it into the OpenAI LLM model for evaluation. The generated output from the model is a `ChatMessage` object.\n",
    "* Finally, the StrOutputParser() component takes in a `ChatMessage` and transforms this into a Python string, which is returned from the `.invoke` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabcdd3",
   "metadata": {},
   "source": [
    "Let's test out the Q&A chain functionality using the `.invoke()` method! In the following code, we will use the `textwrap` package functionality to print the chain response in a more convenient appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c608637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Indonesian Stock Exchange (IDX) was formed on November 30, 2007, through the\n",
      "merger of the Jakarta Stock Exchange (JSX) and the Surabaya Stock Exchange (SSX). It\n",
      "aims to become a credible exchange that drives financial deepening and empowers\n",
      "Indonesia to become the 5th largest economy by 2045. IDX is located at Gedung Bursa\n",
      "Efek Indonesia, Tower I, Jakarta, Indonesia. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "print(textwrap.fill(rag_chain.invoke(\"What is Indonesian Stock Exchange?\"), width = 85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17b6a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IHSG pada 13 September 2022 mencapai level 7.318,016. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(rag_chain.invoke(\"Berapakah nilai IHSG pada 13 September 2022?\"), width = 85))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d4021",
   "metadata": {},
   "source": [
    "## Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce4d39c",
   "metadata": {},
   "source": [
    "To build the summarizer functionality, we will use the `RetrievalQA()` object from the `langchain.chains` module. This chain is used for question-answering against an index. This class extends the functionality of Q&A cases, and we will adapt it to provide a general summary of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c400d",
   "metadata": {},
   "source": [
    "Let's import the required object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4111b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e346c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm = llm,\n",
    "                                       chain_type = \"stuff\",\n",
    "                                       retriever = retriever,\n",
    "                                       return_source_documents = True,\n",
    "                                       verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80f8c5",
   "metadata": {},
   "source": [
    "Pay attention to where we define `qa_chain` above! With the `from_chain_type()` method, we pass the LLM that will generate the summary and the document we want to summarize via the `llm` and `retriever` parameters consecutively. When we declare `chain_type = \"stuff\"`, it takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2053972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASANI\\anaconda3\\envs\\dss_pdf\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chain_result = qa_chain(\"Give me the summary in general!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626099b6",
   "metadata": {},
   "source": [
    "We execute the question-answering chain (`qa_chain`) by passing a question as input. In this case, the question is \"Give me the summary in general!\" The question is formulated based on the user's request for a summary of the document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84c39883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Give me the summary in general!',\n",
       " 'result': 'The summary of the provided context is about the activities and initiatives undertaken by a company, particularly related to organizational development, information dissemination, and system development. It includes details on various divisions, workshops, e-learning sessions, capacity building programs, and information dissemination services. Additionally, there is information on the number of agreements and memorandums issued by the Legal Division, as well as the development phases of a reporting system and special notation module.',\n",
       " 'source_documents': [Document(page_content='membership), as well as for boositng future collaboration. The \\ndetails of the activities are as follows:1. Organization Enabler Part 1:\\n ‚Ä∫Organization & HC Updates;\\n ‚Ä∫Corporate Strategy & Subsidiary Management;\\n ‚Ä∫Corporate Secretary;\\n ‚Ä∫Governance, Risk, and Compliance.\\n2. Organization Enabler Part 2:\\n ‚Ä∫Finance & Accounting Updates;\\n ‚Ä∫General Affair Updates;\\n ‚Ä∫Legal Updates.\\n3. Exchange Business Part 1:\\n ‚Ä∫Member & Participant Management;\\n ‚Ä∫Listing Management.\\n4. Exchange Business Part 2:\\n ‚Ä∫Operational Trading & Market Surveillance;\\n ‚Ä∫Data Management & Services by LDT Division.\\n5. Business Enabler Part 1:\\n ‚Ä∫Business Research & Product Development;\\n ‚Ä∫Market Development;\\n ‚Ä∫Sharia Market Development.\\n6. Business Enabler Part 2:\\n ‚Ä∫\\nIT Management & Services by PTI, OTP , OTD, SDI Divisions;\\n ‚Ä∫Strategy & Architecture;\\n ‚Ä∫IT Business and Innovation;\\n ‚Ä∫IT Application & Infrastructure;\\n ‚Ä∫IT Project Management;\\n ‚Ä∫IT Operations & Security;\\n ‚Ä∫Data Center Management;\\n ‚Ä∫Others.'),\n",
       "  Document(page_content='Division Heads and their Equivalents(Strategic Leader for Division Head)Experienced Sharing Session: How to elevate your career and leadership skills to be ready for the next level\\n2. Virtual Mini Workshop ‚Äì Managing the Ideal Hybrid Workplace (2 batch)\\n3. Mandatory e-learning: Facilitating Change in Organization (Leading Change & Creating a culture of change)\\n4. Mandatory e-learning: Level Up Your Remote Team Experience (Building Connection and Engagement in Virtual Teams, Delegating from a Distance)\\n5. Kepala Unit dan Setara |  Unit Heads and their Equivalents(Operational Leader for Unit Head)Capacity Building for New Unit Head\\n6. 7 Habits for Managers (New Unit Head)\\n7. Mandatory e-learning: Facilitating change in Organization (Developing adaptability as a manager, leading your team through change, communicating in times of change)'),\n",
       "  Document(page_content='Office Memos (‚ÄúNDE‚Äù) related to Agreements, including \\nAddendums and Memorandums of Understanding (MoUs). These \\nNDEs consisted of 283 reviewed NDEs and 736 finalized NDEs \\nthat had previously been reviewed before being finalized or re-\\nfinalized with the same agreement number. The total number of \\nagreements, addendums, and MoUs issued by the Legal Division \\nis 717 numbers.\\nInformation Dissemination Services\\nIn order to provide updates to the internal IDX regarding the \\nFinancial Services Authority Regulations related to the Capital Market and newly issued Exchange Regulations, the Company, \\nthrough the Legal Division, disseminates information by providing summaries of the new or amended provisions of \\nthese Regulations. This enables essential updates and aids in \\nidentifying the aspects that require adjustments within the \\nExchange Regulations.\\nThroughout 2022, the number of information disseminations on \\nFinancial Services Authority Regulations related to the Capital'),\n",
       "  Document(page_content='and Reporting System on IDX so that the process of \\nupdating parameters could be more efficient;\\nc. Development of the Special Notation module with the addition of Dual Language Special Notation on \\nIDX‚Äôs website. This was designed to make the special \\nnotation more easily understood by regional investors. \\nThe addition of the special historical notation module was \\ndesigned to better process and present special notation \\ndata in SPOP+.\\n‚Ä¢ Phase 3 (Live on December 5, 2022)\\nDevelopment in this phase included several things, namely:\\na. Integration of the SPOP+ and Dropzone systems. This \\nwas designed to serve as a medium for sending files \\nbetween Business and Office networks;\\nb. Development of Special Notation so that SPOP+ can \\naccommodate Special Notations N, K, and I related to \\nthe New Economy Board;\\nc. Comparator equity and fixed income designed to make \\ncomparisons between the SPOP+ database and the JATS')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611baa4",
   "metadata": {},
   "source": [
    "When we set `return_source_document = True`, the resulting `chain_result` returns the `source_document`, which signifies the source LLM used to infer a summary. To extract only the response from LLM, subset only the `result` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea1ce16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary of the provided context is about the activities and\n",
      "initiatives undertaken by a company, particularly related to\n",
      "organizational development, information dissemination, and system\n",
      "development. It includes details on various divisions, workshops,\n",
      "e-learning sessions, capacity building programs, and information\n",
      "dissemination services. Additionally, there is information on the\n",
      "number of agreements and memorandums issued by the Legal Division, as\n",
      "well as the development phases of a reporting system and special\n",
      "notation module.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(chain_result['result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04977afc",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this coursebook, we have learned the introductory concepts of Large Language Models (LLMs) and explored how we can extend their capabilities to text-based specific use cases, such as PDF Q&A and summarization. We have delved into the workflows for using LLMs for these cases, starting from text preprocessing, embedding the text, and storing the embeddings in a vector database. Finally, we provided these embeddings as context for the LLM, enabling effective information retrieval and the extraction of a general summary of the document.\n",
    "\n",
    "Lastly, this coursebook has provided a brief overview of LLM implementation for unstructured data. Find more resources in further readings to explore other astonishing implementations of LLMs using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a64008",
   "metadata": {},
   "source": [
    "# Further Readings\n",
    "\n",
    "* [LangChain AI Handbook](https://www.pinecone.io/learn/series/langchain/).\n",
    "* [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pro_xl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
